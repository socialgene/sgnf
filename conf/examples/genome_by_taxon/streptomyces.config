/*
========================================================================================
    Nextflow config file for running minimal tests
========================================================================================
    Defines input files and everything required to run a fast and simple pipeline test.

    Use as follows:

        outdir='/home/chase/Documents/socialgene_data/jgi_steptomyces_pangenome'
        outdir_download_cache='/home/chase/Documents/socialgene_data/cache'

        nextflow run . \
            -profile streptomyces,docker \
            --outdir $outdir \
            --outdir_download_cache $outdir_download_cache \
            --fasta_splits 1000 \
            --htcondor \
            -resume

    # The second parses the HTCONDOR results and then all other computation (e.g. BLASTp, MMseqs2)
        nextflow run . \
            -profile streptomyces,docker \
            --outdir $outdir \
            --outdir_download_cache $outdir_download_cache \
            --domtblout_path '/home/chase/Documents/socialgene_data/jgi_steptomyces_pangenome/htcondor_cache/chtc_results/*.domtblout.gz' \
            -resume


------------------------------------------------------
  outdir='/home/chase/Documents/socialgene_data/streptomyces'

  nextflow run . \
    -profile streptomyces,docker \
    --outdir $outdir \
    -resume



----------------------------------------------------------------------------------------
*/

params {
    local_genbank = '/home/chase/Documents/socialgene_data/jgi_paper/ncbi-genomes-2022-11-09/*.gbff.gz'


    /*
    ////////////////////////
    set which modules to run
    ////////////////////////
    */
        mibig           = false
        hmmlist         = "all"
        mmseqs2         = true
        blastp          = true
        ncbi_taxonomy   = true
        build_database  = true
        antismash       = true

    /*

    ////////////////////////
    Adjust per your computer
    ////////////////////////
    */
        // If running the pipelie on a single computer, fasta_splits should be set at the same
        // number as max_cpus. For very large input data you might
        // want to set fasta_splits to a higher number to allow checkpointing. Howevever, setting
        // fasta_splits too high will result in diminishing returns due to overhead and possibly
        // too-few proteins per file (would probably keep <1000 for any size of dataset)
        fasta_splits                = 24
        max_cpus                    = 24
        max_memory                  = '60.GB'
        max_time                    = 6000.h

}

process {

    withName:DIAMOND_BLASTP {
        cpus = 24
        ext.args = "-k0 --max-hsps 1 --query-cover 70 --subject-cover 70 --id 80  --fast"
    }
}
