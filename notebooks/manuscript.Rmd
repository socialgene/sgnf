


# Stages of discvering a BGC





https://pubs.rsc.org/en/content/articlehtml/2021/np/d0np00055h

When analyzing a new BGC researchers often turn to running individual BLAST searches against the comprehensive NCBI database (nr.gz).

Popular rule-based tools such as PRISM and antiSMASH, and machine learning algorithms like DeepBGC, find BGs either through defined patterns or self-learned training.

While rule-based or machine learning constructs may be constructed from socialgene and/or its underlying data, it fulfills two other gaps in the annotation and knowledge discovery pipeline.

Hypothesis-driven evolutionary questions:

1) Provided a BGC, how do its genes relate to the context of all other sequenced organisms?
2) How do known BGCs relate to each other and how do they relate across taxonomy?
3) How doe genes and domains relate to each other across BGCs.

Practical BGC discovery:

1) What genomes have BGCs similar to mine? And are any of these available in a strain collection?

# Reducing data complexity


Oversimplifying, ML algorithms and user-facing database systems often have the opposing priorities of speed and accuracy. An increase in one often comes at the expense of the other.

While the full-sized socialgene database is relatively compact for its information richness, it is still larger than what common drug discovery laboratories are used to handling in-lab on a day-to-day basis. It is also quite a heavy database to deploy, with disk and RAM usage making cloud-deployment expensive.

For this reason socialgene-lite was also created to attempt to minimize information-loss while creating a database size that is more distributable and amenable as an online endpoint. Instead of using domain positions, socialgene-lite only contains and searches domain content, irrespective of domain order/position. This results in less-accurate (and sometimes inaccurate) comparisons but with an immense trade-off in database size and speed.

socialgene-medium sits between the full-sized database and socialgene-lite. It includes approximations of the order of domains, which allows grouping many non-redunandant proteins into a smaller set of related proteins.






First pass -> hash protein and see if identical protein is in database
Second pass -> for non-identical, hmmsearch against same set of hmms as database






There are many methods by which socialgene could balance information loss and size/speed of the database.

- Use non-redundant set of proteins
- Use non-redundant set of hmm models

Smallest to largest:
1) Collapse proteins that have identical presence/absence of domains
2) Collapse proteins that have identical domain order
3) Collapse proteins that have highly-similar domain structure (distance on length)

- 1 & 2 can be computed using the same vector of hmms
  - Assign 1,2,3, etc based on order.
    - For #1, change to binary vector, group identical vectors
        - Could be "tuned" using domain hit e-values
        - COuld be tuned by using # of intra-protein domain occurrences as well
    - For #2, group identical vectors



Don't have to make full vector for #1. Could sort, concatenate string, then hash.




- for #1
  - Filter resulting hmm hits
  - Sort hits alphabetically
  - Concatenate hmm name strings
  - hash

- for #2
  - Filter resulting hmm hits
  - Sort hits based on middle of domain location
  - Concatenate hmm name strings
  - hash


Make network from representative of groups


socialgene and socialgene-lite
